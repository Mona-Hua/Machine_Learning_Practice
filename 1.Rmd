---
title: "Untitled"
author: "Bingqing Hua"
date: "2021/5/21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(xgboost)

setwd("D:/monash/etc3250ML/project")
# Read data


```

```{r}
# You also need to install xgboost using install.packages('xgboost') to run this script.

# For reproducibility
set.seed(32505250)

# Load in the training data
mydat <- read_csv("student_training_release.csv") 

# Define our model. Here we use a xgboost model. You could also choose to use other models.
#
# We would like to choose the parameter `mtry`, `tress` and `tree_depth` using CV. Thus,
# we assign them with the placeholder `tune()`. Then, we set "xgboost" as our engine.
# Use help("boost_tree") to check the main arguments.
mymodel <- boost_tree(mtry = tune(),
                      trees = tune(),
                      tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Define our CV process. Here we use a 3-fold CV.
mycv <- vfold_cv(mydat, v = 3, strata = "cause")


# Define our recipe. Here we use cause as our response variable and 9 other variables
# as our predictors. We first transform all the character variables to factor variables by
# using step_string2factor(). Then, since there are some missing values in the data,
# we use the knn imputation algorithm to replace NA.
# However, the parameter k is unknown and we would like it to be tuned by CV.
# The final step is to transform all the nominal predictors to dummy variables.
# The reason to perform this step is because xgboost doesn't accept categorical
# variable. You might not need to use this step for other models.
# If you feel like the knn imputation takes a significant amount time to run, you could try other imputation
# methods provided by recipes.
# https://recipes.tidymodels.org/reference/index.html
# There are also other preprocessing steps you could try.
myrecipe <- recipe(mydat, formula = cause ~ 
                     ws + rf + se + maxt + mint + day + FOR_TYPE + COVER + HEIGHT) %>%
  step_string2factor(all_nominal_predictors()) %>%
  step_knnimpute (all_predictors(), neighbors = tune()) %>%
  step_dummy(all_nominal_predictors())


# Define our workflow by adding the data preprocessing steps and model specification.
myworkflow <- workflow() %>%
  add_recipe(myrecipe) %>%
  add_model(mymodel)

# Define our tuning grid. These values are randomly chosen by me. You need to make your own
# decision on this grid.
mygrid <- expand.grid(mtry = c(5, 7, 9),
                      neighbors = c(3, 5),
                      trees = seq(200, 1000, 200),
                      tree_depth = c(5, 7, 9))


# Define the tuning controller. I want some feedback during the training, so I turn on the verbose.
# You could set it to FALSE.
mycontrol <- control_grid(verbose = TRUE)

# Tune our parameters by using tune_grid(). In this function, we also need to specify the
# resampling process and the tuning grid. The control is an optional argument.
# Generally, parameter tuning using CV is time consuming.
grid_result <- myworkflow %>%
  tune_grid(resamples = mycv, grid = mygrid, control = mycontrol)

# plot the tuning result
autoplot(grid_result)

# From the tuning result, we need to choose the best one by providing the metric that you
# are interested in. The return of select_best() is a tibble.
mybestpara <- select_best(grid_result, metric = "accuracy")

# Using the given tibble, we can finalize our workflow. Basically, it replaces the
# placeholder tune() with the best values. Now, we have a complete but untrained workflow.
myworkflow <- myworkflow %>%
  finalize_workflow(mybestpara)

# The last thing we need to do is to fit our complete workflow with our data. Then, we will
# have a trained workflow which can be used in prediction.
myworkflow <- myworkflow %>%
  fit(data = mydat)

# Load in the prediction data.
preddat <- read_csv(here::here("data/release/student/student_predict_x_release.csv"))

# Obtaining the prediction from a workflow is easy. But you may concern about the data
# transformation, the knn imputation and all other data preprocessing steps you apply on
# the training set before you train the model. Will the workflow apply the same operations
# on the prediction set? The answer is YES. The workflow will preprocess the prediction set
# with the same recipe, and then use the trained model to predict values.
mypred <- myworkflow %>%
  predict(preddat)

# Save our prediction and ready to submit
data.frame(Category = mypred$.pred_class) %>%
  mutate(Id = 1:n()) %>%
  select(Id, Category) %>%
  write_csv("__test__3.csv")

```

